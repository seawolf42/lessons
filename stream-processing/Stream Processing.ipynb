{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing\n",
    "\n",
    "A very common need in many languages is the ability to process streams of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "To follow what's going on here, we'll use a decorator to print out the output of every stage of our processing pipelines. If you're not familiar with decorators, you can look into one of the many tutorials on the matter (including [my decorators tutorial](../decorators/README.ipynb)). All this decorator does is print out the function name and value of each piece of data passing through it. This gives us visibility into what's going on inside various generators that are processing concurrently.\n",
    "\n",
    "The other function we'll use is just a simple function that iterates over the stream as a whole and emits each piece of final output. This is the actual output from whatever stream pipeline we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_stream: 0\n",
      "output: 0\n",
      "basic_stream: 1\n",
      "output: 1\n",
      "basic_stream: 2\n",
      "output: 2\n",
      "output: ... (ignoring the rest)\n"
     ]
    }
   ],
   "source": [
    "def print_element(f_name, item):\n",
    "    print(f'{f_name}: {item}')\n",
    "\n",
    "def print_elements(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        f_name = func.__name__\n",
    "        for item in func(*args, **kwargs):\n",
    "            print(f'{f_name}: {item}')\n",
    "            yield item\n",
    "    return wrapper\n",
    "\n",
    "def consume(stream, max_len=None):\n",
    "    index = 0\n",
    "    for item in stream:\n",
    "        print(f'output: {item}')\n",
    "        index += 1\n",
    "        if max_len is not None and index >= max_len:\n",
    "            print('output: ... (ignoring the rest)')\n",
    "            break\n",
    "\n",
    "@print_elements\n",
    "def basic_stream(n=10):\n",
    "    return range(n)\n",
    "\n",
    "consume(basic_stream(), max_len=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything that is iterable can be passed to `consume()`. In this quick dump, we emit a string (which iterates as a series of characters) so we can see how the output handles whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: a\n",
      "output: b\n",
      "output: \t\n",
      "output: c\n",
      "output: \n",
      "\n",
      "output: d\n",
      "output:  \n",
      "output: e\n"
     ]
    }
   ],
   "source": [
    "consume('ab\\tc\\nd e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will also need a stream of data to process. For simplicity, I will be using a generator that simply emits lines from a pretend file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_strings: the\n",
      "output: the\n",
      "source_strings: quick\n",
      "output: quick\n",
      "source_strings: brown\n",
      "output: brown\n",
      "source_strings: fox\n",
      "output: fox\n",
      "source_strings: jumps\n",
      "output: jumps\n",
      "source_strings: over\n",
      "output: over\n",
      "source_strings: the\n",
      "output: the\n",
      "source_strings: lazy\n",
      "output: lazy\n",
      "source_strings: dog\n",
      "output: dog\n"
     ]
    }
   ],
   "source": [
    "@print_elements\n",
    "def source_strings():\n",
    "    return 'the quick brown fox jumps over the lazy dog'.split()\n",
    "\n",
    "consume(source_strings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "The basic idea of a pipeline is that you have a series of iterables, each of which consumes a stream (its input) and emits a stream (its output). The last example was techincally a pipeline.\n",
    "\n",
    "Another example would be to normalize our input, perhaps by capitalizing each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_strings: the\n",
      "to_upper: THE\n",
      "output: THE\n",
      "source_strings: quick\n",
      "to_upper: QUICK\n",
      "output: QUICK\n",
      "source_strings: brown\n",
      "to_upper: BROWN\n",
      "output: BROWN\n",
      "output: ... (ignoring the rest)\n"
     ]
    }
   ],
   "source": [
    "@print_elements\n",
    "def to_upper(word_stream):\n",
    "    for word in word_stream:\n",
    "        yield word.upper()\n",
    "\n",
    "consume(to_upper(source_strings()), max_len=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex example might only modify certain words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_strings: the\n",
      "to_upper_conditional: the\n",
      "output: the\n",
      "source_strings: quick\n",
      "to_upper_conditional: quick\n",
      "output: quick\n",
      "source_strings: brown\n",
      "to_upper_conditional: BROWN\n",
      "output: BROWN\n",
      "source_strings: fox\n",
      "to_upper_conditional: FOX\n",
      "output: FOX\n",
      "source_strings: jumps\n",
      "to_upper_conditional: jumps\n",
      "output: jumps\n",
      "output: ... (ignoring the rest)\n"
     ]
    }
   ],
   "source": [
    "@print_elements\n",
    "def to_upper_conditional(word_stream):\n",
    "    for word in word_stream:\n",
    "        yield word.upper() if word[0] in 'abcdef' else word\n",
    "\n",
    "consume(to_upper_conditional(source_strings()), max_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators\n",
    "\n",
    "If we want to really do great things with pipelines, we should be using generators and generator functions. \n",
    "\n",
    "The first thing to realize about generators is that they advance only when they need to. For instance, if we have one step of our processing that needs to break strings into characters, each generator in the pipeline only reads the next element when it is explicitly requested by a stage further down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_strings: the\n",
      "convert_strings_to_characters: t\n",
      "output: t\n",
      "convert_strings_to_characters: h\n",
      "output: h\n",
      "convert_strings_to_characters: e\n",
      "output: e\n",
      "source_strings: quick\n",
      "convert_strings_to_characters: q\n",
      "output: q\n",
      "convert_strings_to_characters: u\n",
      "output: u\n",
      "convert_strings_to_characters: i\n",
      "output: i\n",
      "convert_strings_to_characters: c\n",
      "output: c\n",
      "convert_strings_to_characters: k\n",
      "output: k\n",
      "source_strings: brown\n",
      "convert_strings_to_characters: b\n",
      "output: b\n",
      "output: ... (ignoring the rest)\n"
     ]
    }
   ],
   "source": [
    "@print_elements\n",
    "def convert_strings_to_characters(word_stream):\n",
    "    for word in word_stream:\n",
    "        for character in word:\n",
    "            yield character\n",
    "\n",
    "consume(convert_strings_to_characters(source_strings()), max_len=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing here is that the inner-most generator (`source_strings()`) is only asked to return the k<sup>th</sup> word after all of the (k-1)<sup>th</sup> word has been consumed. At any point in time, only a small amount of memory is actually in use for data within the pipeline, so if the source input and ultimate output are outside of memory (a database, a file, a network connection, whatever) then memory use of this pipeline is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompressing content\n",
    "\n",
    "Let's imagine we have a compressed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "She sells sea shells by the seashore.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "file_name = 'content.txt.gz'\n",
    "\n",
    "with gzip.open(file_name) as fin:\n",
    "    content = fin.read()\n",
    "    print(content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when you call `fin.read()`, the *entire* file is being read into memory. With very large files, this is a problem!\n",
    "\n",
    "For the remainder of this tutorial, we will be working with a stream of `bytestring` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_stream: b'\\x1f\\x8b\\x08\\x08\\x96\\xabM^\\x00\\x03content.txt\\x00\\x1d\\x8bA\\x12@0\\x14C\\xf7='\n",
      "content_stream: b'E\\\\\\xa0\\xe7\\xb0g\\xc6\\xba\\xd5R\\x94O\\xbf\\xa2N\\xef\\x8fM&yIj\\x1f#\\xa1\\xa3\\x14]\\xa5\\xda\\xe0q'\n",
      "content_stream: b'\\xe4\\xa9_`\\x13\\xdd\\x1b\\x06z0\\xe7ug\\xd0\\xe5\\x13N\\xa9\\xa3y\\x0b\\x1c\\x8dZ5\\x92X\\xde,j\\xc0\\xe1'\n",
      "content_stream: b'\\xb7\\xb6\\xfc+!\\x1c(y\\xad>\\xe4\\xae)\\xe8`\\x00\\x00\\x00'\n",
      "content_stream: b'\\x1f\\x8b\\x08\\x08\\x96\\xabM^\\x00\\x03content.txt\\x00\\x1d\\x8bA\\x12@0\\x14C\\xf7='\n",
      "output: b'\\x1f\\x8b\\x08\\x08\\x96\\xabM^\\x00\\x03content.txt\\x00\\x1d\\x8bA\\x12@0\\x14C\\xf7='\n",
      "content_stream: b'E\\\\\\xa0\\xe7\\xb0g\\xc6\\xba\\xd5R\\x94O\\xbf\\xa2N\\xef\\x8fM&yIj\\x1f#\\xa1\\xa3\\x14]\\xa5\\xda\\xe0q'\n",
      "output: b'E\\\\\\xa0\\xe7\\xb0g\\xc6\\xba\\xd5R\\x94O\\xbf\\xa2N\\xef\\x8fM&yIj\\x1f#\\xa1\\xa3\\x14]\\xa5\\xda\\xe0q'\n",
      "content_stream: b'\\xe4\\xa9_`\\x13\\xdd\\x1b\\x06z0\\xe7ug\\xd0\\xe5\\x13N\\xa9\\xa3y\\x0b\\x1c\\x8dZ5\\x92X\\xde,j\\xc0\\xe1'\n",
      "output: b'\\xe4\\xa9_`\\x13\\xdd\\x1b\\x06z0\\xe7ug\\xd0\\xe5\\x13N\\xa9\\xa3y\\x0b\\x1c\\x8dZ5\\x92X\\xde,j\\xc0\\xe1'\n",
      "content_stream: b'\\xb7\\xb6\\xfc+!\\x1c(y\\xad>\\xe4\\xae)\\xe8`\\x00\\x00\\x00'\n",
      "output: b'\\xb7\\xb6\\xfc+!\\x1c(y\\xad>\\xe4\\xae)\\xe8`\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "with open(file_name, 'rb') as fin:\n",
    "    content = fin.read()\n",
    "\n",
    "@print_elements\n",
    "def content_stream(chunk_size=32):\n",
    "    return (content[i:i+chunk_size] for i in range(0, len(content), chunk_size))\n",
    "\n",
    "# the following line can be used to confirm that if you re-join the\n",
    "# chunks, you get back the original file contents:\n",
    "# assert b''.join(content_stream()) == content\n",
    "\n",
    "consume(content_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are no longer working with a file, we can't use the `gzip` library any longer. Instead we can use `zlib` to decode in chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_stream: b'\\x1f\\x8b\\x08\\x08\\x96\\xabM^\\x00\\x03content.txt\\x00\\x1d\\x8bA\\x12@0\\x14C\\xf7=E\\\\\\xa0\\xe7\\xb0g\\xc6\\xba\\xd5R\\x94O\\xbf\\xa2N\\xef\\x8fM&yIj\\x1f#\\xa1\\xa3\\x14]\\xa5\\xda\\xe0q'\n",
      "decode_gzip_stream: b'Hello World!\\nThe '\n",
      "output: b'Hello World!\\nThe '\n",
      "content_stream: b'\\xe4\\xa9_`\\x13\\xdd\\x1b\\x06z0\\xe7ug\\xd0\\xe5\\x13N\\xa9\\xa3y\\x0b\\x1c\\x8dZ5\\x92X\\xde,j\\xc0\\xe1\\xb7\\xb6\\xfc+!\\x1c(y\\xad>\\xe4\\xae)\\xe8`\\x00\\x00\\x00'\n",
      "decode_gzip_stream: b'quick brown fox jumps over the lazy dog.\\nShe sells sea shells by the seashore.\\n'\n",
      "output: b'quick brown fox jumps over the lazy dog.\\nShe sells sea shells by the seashore.\\n'\n"
     ]
    }
   ],
   "source": [
    "import zlib\n",
    "\n",
    "@print_elements\n",
    "def decode_gzip_stream(stream):\n",
    "    decompressor = zlib.decompressobj(zlib.MAX_WBITS | 16)\n",
    "    return (decompressor.decompress(chunk) for chunk in stream)\n",
    "\n",
    "consume(decode_gzip_stream(content_stream(chunk_size=64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of `content_stream` is big in the first chunk and small in the second but that most of the decompressed output doesn't appear until the second compressed chunk has been read; this is because the first part of the file contains metadata that `zlib.decompress` ignores and also because decoding doesn't happen a character at a time but rather includes state that carries from character to character. The output of `zlib.decompress` is variable-length chunks of decompressed data. But since those uncompressed chunks are themselves `bytestring` objects, we can just pass them on down the pipeline without worrying about fixing sizes or anything.\n",
    "\n",
    "Finally, let's say we want to put the decompressed data into fixed-size chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_stream: b'\\x1f\\x8b\\x08\\x08\\x96\\xabM^\\x00\\x03content.txt\\x00\\x1d\\x8bA\\x12@0\\x14C\\xf7='\n",
      "decode_gzip_stream: b''\n",
      "content_stream: b'E\\\\\\xa0\\xe7\\xb0g\\xc6\\xba\\xd5R\\x94O\\xbf\\xa2N\\xef\\x8fM&yIj\\x1f#\\xa1\\xa3\\x14]\\xa5\\xda\\xe0q'\n",
      "decode_gzip_stream: b'Hello World!\\nThe '\n",
      "byte_stream_from_chunks: 72\n",
      "byte_stream_from_chunks: 101\n",
      "byte_stream_from_chunks: 108\n",
      "byte_stream_from_chunks: 108\n",
      "byte_stream_from_chunks: 111\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 87\n",
      "byte_stream_from_chunks: 111\n",
      "fixed_size_chunks: b'Hello Wo'\n",
      "convert_to_strings: Hello Wo\n",
      "output: Hello Wo\n",
      "byte_stream_from_chunks: 114\n",
      "byte_stream_from_chunks: 108\n",
      "byte_stream_from_chunks: 100\n",
      "byte_stream_from_chunks: 33\n",
      "byte_stream_from_chunks: 10\n",
      "byte_stream_from_chunks: 84\n",
      "byte_stream_from_chunks: 104\n",
      "byte_stream_from_chunks: 101\n",
      "fixed_size_chunks: b'rld!\\nThe'\n",
      "convert_to_strings: rld!\n",
      "The\n",
      "output: rld!\n",
      "The\n",
      "byte_stream_from_chunks: 32\n",
      "content_stream: b'\\xe4\\xa9_`\\x13\\xdd\\x1b\\x06z0\\xe7ug\\xd0\\xe5\\x13N\\xa9\\xa3y\\x0b\\x1c\\x8dZ5\\x92X\\xde,j\\xc0\\xe1'\n",
      "decode_gzip_stream: b'quick brown fox jumps over the lazy dog.\\nShe sells sea sh'\n",
      "byte_stream_from_chunks: 113\n",
      "byte_stream_from_chunks: 117\n",
      "byte_stream_from_chunks: 105\n",
      "byte_stream_from_chunks: 99\n",
      "byte_stream_from_chunks: 107\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 98\n",
      "fixed_size_chunks: b' quick b'\n",
      "convert_to_strings:  quick b\n",
      "output:  quick b\n",
      "byte_stream_from_chunks: 114\n",
      "byte_stream_from_chunks: 111\n",
      "byte_stream_from_chunks: 119\n",
      "byte_stream_from_chunks: 110\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 102\n",
      "byte_stream_from_chunks: 111\n",
      "byte_stream_from_chunks: 120\n",
      "fixed_size_chunks: b'rown fox'\n",
      "convert_to_strings: rown fox\n",
      "output: rown fox\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 106\n",
      "byte_stream_from_chunks: 117\n",
      "byte_stream_from_chunks: 109\n",
      "byte_stream_from_chunks: 112\n",
      "byte_stream_from_chunks: 115\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 111\n",
      "fixed_size_chunks: b' jumps o'\n",
      "convert_to_strings:  jumps o\n",
      "output:  jumps o\n",
      "byte_stream_from_chunks: 118\n",
      "byte_stream_from_chunks: 101\n",
      "byte_stream_from_chunks: 114\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 116\n",
      "byte_stream_from_chunks: 104\n",
      "byte_stream_from_chunks: 101\n",
      "byte_stream_from_chunks: 32\n",
      "fixed_size_chunks: b'ver the '\n",
      "convert_to_strings: ver the \n",
      "output: ver the \n",
      "byte_stream_from_chunks: 108\n",
      "byte_stream_from_chunks: 97\n",
      "byte_stream_from_chunks: 122\n",
      "byte_stream_from_chunks: 121\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 100\n",
      "byte_stream_from_chunks: 111\n",
      "byte_stream_from_chunks: 103\n",
      "fixed_size_chunks: b'lazy dog'\n",
      "convert_to_strings: lazy dog\n",
      "output: lazy dog\n",
      "byte_stream_from_chunks: 46\n",
      "byte_stream_from_chunks: 10\n",
      "byte_stream_from_chunks: 83\n",
      "byte_stream_from_chunks: 104\n",
      "byte_stream_from_chunks: 101\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 115\n",
      "byte_stream_from_chunks: 101\n",
      "fixed_size_chunks: b'.\\nShe se'\n",
      "convert_to_strings: .\n",
      "She se\n",
      "output: .\n",
      "She se\n",
      "byte_stream_from_chunks: 108\n",
      "byte_stream_from_chunks: 108\n",
      "byte_stream_from_chunks: 115\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 115\n",
      "byte_stream_from_chunks: 101\n",
      "byte_stream_from_chunks: 97\n",
      "byte_stream_from_chunks: 32\n",
      "fixed_size_chunks: b'lls sea '\n",
      "convert_to_strings: lls sea \n",
      "output: lls sea \n",
      "byte_stream_from_chunks: 115\n",
      "byte_stream_from_chunks: 104\n",
      "content_stream: b'\\xb7\\xb6\\xfc+!\\x1c(y\\xad>\\xe4\\xae)\\xe8`\\x00\\x00\\x00'\n",
      "decode_gzip_stream: b'ells by the seashore.\\n'\n",
      "byte_stream_from_chunks: 101\n",
      "byte_stream_from_chunks: 108\n",
      "byte_stream_from_chunks: 108\n",
      "byte_stream_from_chunks: 115\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 98\n",
      "fixed_size_chunks: b'shells b'\n",
      "convert_to_strings: shells b\n",
      "output: shells b\n",
      "byte_stream_from_chunks: 121\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 116\n",
      "byte_stream_from_chunks: 104\n",
      "byte_stream_from_chunks: 101\n",
      "byte_stream_from_chunks: 32\n",
      "byte_stream_from_chunks: 115\n",
      "byte_stream_from_chunks: 101\n",
      "fixed_size_chunks: b'y the se'\n",
      "convert_to_strings: y the se\n",
      "output: y the se\n",
      "byte_stream_from_chunks: 97\n",
      "byte_stream_from_chunks: 115\n",
      "byte_stream_from_chunks: 104\n",
      "byte_stream_from_chunks: 111\n",
      "byte_stream_from_chunks: 114\n",
      "byte_stream_from_chunks: 101\n",
      "byte_stream_from_chunks: 46\n",
      "byte_stream_from_chunks: 10\n",
      "fixed_size_chunks: b'ashore.\\n'\n",
      "convert_to_strings: ashore.\n",
      "\n",
      "output: ashore.\n",
      "\n",
      "fixed_size_chunks: b''\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "@print_elements\n",
    "def byte_stream_from_chunks(chunk_stream):\n",
    "    for chunk in chunk_stream:\n",
    "        for byte in chunk:\n",
    "            yield byte\n",
    "\n",
    "@print_elements\n",
    "def fixed_size_chunks(byte_stream, chunk_size=8):\n",
    "    chunk = []\n",
    "    for byte in byte_stream:\n",
    "        chunk.append(byte)\n",
    "        if len(chunk) == chunk_size:\n",
    "            yield bytes(chunk)\n",
    "            chunk = []\n",
    "    yield bytes(chunk)\n",
    "\n",
    "@print_elements\n",
    "def convert_to_strings(chunk_stream):\n",
    "    return codecs.iterdecode(chunk_stream, 'utf-8')\n",
    "\n",
    "consume(\n",
    "    convert_to_strings(\n",
    "        fixed_size_chunks(\n",
    "            byte_stream_from_chunks(\n",
    "                decode_gzip_stream(\n",
    "                    content_stream()\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a whole lot going on there... let's break it down:\n",
    "\n",
    "1. The byte stream produced by the innermost function simply emits fixed-size chunks of bytes from the source content\n",
    "1. These chunks are passed to a decoder that decompresses as much as it can from each chunk\n",
    "    1. at the end of a chunk, the still-uncompressed portion becomes the first part of the next chunk to be decompressed\n",
    "1. The decompressed chunks are turned into a single stream of bytes\n",
    "    1. **NOTE**: when you iterate a `str` object, the output is a sequence of characters, but when you iterate a `bytes` object, the output is a sequence of *integers* (`int` objects); this is why you see numbers in the output of `byte_stream_from_chunks`; later in `fixed_size_chunks`, calling `bytes(sequence_of_ints)` returns a bytestring again, so we just need to be aware of the change of type so we aren't confused by errors or bugs related to the change\n",
    "1. The sequence of individual bytes are grouped into fixed-size chunks and passed down the pipeline\n",
    "1. each fixed-size chunk is converted to a `str` object (using the `codecs` library)\n",
    "\n",
    "You can see in the output that the first chunk is received, then subsets of that data are emitted as requested by each next stage in the pipeline. In particular, look at the following sequence in the output:\n",
    "\n",
    "    byte_stream_from_chunks: 104\n",
    "    byte_stream_from_chunks: 101\n",
    "    fixed_size_chunks: b'rld!\\nThe'\n",
    "    convert_to_strings: rld!\n",
    "    The\n",
    "    output: rld!\n",
    "    The\n",
    "    byte_stream_from_chunks: 32\n",
    "    content_stream: b'\\xe4\\xa9_`\\x13\\xdd\\x1b\\x06z0\\xe7ug\\xd0\\xe5\\x13N\\xa9\\xa3y\\x0b\\x1c\\x8dZ5\\x92X\\xde,j\\xc0\\xe1'\n",
    "    decode_gzip_stream: b'quick brown fox jumps over the lazy dog.\\nShe sells sea sh'\n",
    "    byte_stream_from_chunks: 113\n",
    "    byte_stream_from_chunks: 117\n",
    "\n",
    "`fixed_size_chunks` emits `b'rld!\\nThe'` (which is converted to text and emitted in the output), then continues collecting bytes for the next chunk. It is able to get one (`byte_stream_from_chunks: 32`), but then `byte_stream_from_chunks` has used everything in the current chunk and needs the next one. This causes `decode_gzip_stream` to have to grab the next chunk from the source stream, so we see that happen before `byte_stream_from_chunks` is able to continue feeding values to `fixed_size_chunks`.\n",
    "\n",
    "At no point does any part of the pipeline depend on the entire file being in memory. If our file were one petabyte in size, then as long as we read it into the computer in small chunks we would only use enough memory to store the current chunk of raw data plus some intermediate state in each function in the pipeline (this isn't entirely true, see the note below).\n",
    "\n",
    "**Note**: we're ignoring a lot of nuance around how memory is reclaimed in garbage collection here, but the point is that we would never need more than a minimal amout of RAM; this should run successfully on any computer that can run Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Generators](https://wiki.python.org/moin/Generators)\n",
    "* [Primer on Python Decorators](https://realpython.com/primer-on-python-decorators/)\n",
    "* [io â€” Core tools for working with streams](https://docs.python.org/3/library/io.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
