{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Map/Reduce with Python\n",
    "\n",
    "In this module we are going to look at how to solve problems using the Map/Reduce paradigm. We will solve for `pi` in possibly the most inefficient but fun way: throwing darts at a dartboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Map/Reduce?\n",
    "\n",
    "Map/Reduce is a method of distributing processing across several CPUs. It shares concepts from multithreading on CPUs and parallel processing on GPUs but with one major difference: tasks are parallelized across machines in a cluster, and task coordination is such that compute units can be added or removed during a job without affecting the outcome. This means jobs can be scaled as-needed limited only by the availability of more computers.\n",
    "\n",
    "The basic idea of Map/Reduce is that most tasks can be split into two phases: a data filter/select/project (**map**) phase that is highly parallelizable and an aggregation (**reduce**) phase which is less so. Since the map phase involves the most heavy lifting and often involves loading each record of interest no matter how large the data set, it is spread across a cluster and only summarized or interim results are returned and processed further. The reduce phase involves collecting the raw output from mapping and collating or summarizing that data into values that directly or indirectly answer the question at hand.\n",
    "\n",
    "### Example\n",
    "\n",
    "The canonical example is counting words in a set of documents. Assuming the documents look like this:\n",
    "\n",
    "> the quick brown fox jumps over the lazy dog\n",
    "> \n",
    "> she sells sea shells by the seashore\n",
    "> \n",
    "> how much wood would a woodchuck chuck if a woodchuck could chuck wood\n",
    "\n",
    "... we want to get something like this:\n",
    "\n",
    "| word | count |\n",
    "|---|---|\n",
    "| a | 2 |\n",
    "| brown | 1 |\n",
    "| by | 1 |\n",
    "| chuck | 2 |\n",
    "| ... | ... |\n",
    "| shells | 1 |\n",
    "| the | 3 |\n",
    "| wood | 2 |\n",
    "| woodchuck | 2 |\n",
    "| would | 1 |\n",
    "\n",
    "Think for a moment how you might solve this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 2)\n",
      "('brown', 1)\n",
      "('by', 1)\n",
      "('chuck', 2)\n",
      "('could', 1)\n",
      "('dog', 1)\n",
      "('fox', 1)\n",
      "('how', 1)\n",
      "('if', 1)\n",
      "('jumps', 1)\n",
      "('lazy', 1)\n",
      "('much', 1)\n",
      "('over', 1)\n",
      "('quick', 1)\n",
      "('sea', 1)\n",
      "('seashore', 1)\n",
      "('sells', 1)\n",
      "('she', 1)\n",
      "('shells', 1)\n",
      "('the', 3)\n",
      "('wood', 2)\n",
      "('woodchuck', 2)\n",
      "('would', 1)\n"
     ]
    }
   ],
   "source": [
    "# rough solution to get the idea\n",
    "\n",
    "text = '''the quick brown fox jumps over the lazy dog she\n",
    "sells sea shells by the seashore how much wood would a\n",
    "woodchuck chuck if a woodchuck could chuck wood'''\n",
    "\n",
    "counts_per_word = {}\n",
    "for word in text.split():\n",
    "    if word not in counts_per_word:\n",
    "        counts_per_word[word] = 0\n",
    "    counts_per_word[word] += 1\n",
    "\n",
    "for word, count in sorted(counts_per_word.items()):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were going to spread this across thousands of machines, we have to make a small tweak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('quick', 1),\n",
       " ('brown', 1),\n",
       " ('fox', 1),\n",
       " ('jumps', 1),\n",
       " ('over', 1),\n",
       " ('the', 1),\n",
       " ('lazy', 1),\n",
       " ('dog', 1),\n",
       " ('she', 1),\n",
       " ('sells', 1),\n",
       " ('sea', 1),\n",
       " ('shells', 1),\n",
       " ('by', 1),\n",
       " ('the', 1),\n",
       " ('seashore', 1),\n",
       " ('how', 1),\n",
       " ('much', 1),\n",
       " ('wood', 1),\n",
       " ('would', 1),\n",
       " ('a', 1),\n",
       " ('woodchuck', 1),\n",
       " ('chuck', 1),\n",
       " ('if', 1),\n",
       " ('a', 1),\n",
       " ('woodchuck', 1),\n",
       " ('could', 1),\n",
       " ('chuck', 1),\n",
       " ('wood', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: emit a 1 for every single word found\n",
    "word_instances = []\n",
    "for word in text.split():\n",
    "    word_instances.append((word, 1))\n",
    "\n",
    "# note that we see repeated words emitted once for each occurance in the input\n",
    "word_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 1]),\n",
       " ('brown', [1]),\n",
       " ('by', [1]),\n",
       " ('chuck', [1, 1]),\n",
       " ('could', [1]),\n",
       " ('dog', [1]),\n",
       " ('fox', [1]),\n",
       " ('how', [1]),\n",
       " ('if', [1]),\n",
       " ('jumps', [1]),\n",
       " ('lazy', [1]),\n",
       " ('much', [1]),\n",
       " ('over', [1]),\n",
       " ('quick', [1]),\n",
       " ('sea', [1]),\n",
       " ('seashore', [1]),\n",
       " ('sells', [1]),\n",
       " ('she', [1]),\n",
       " ('shells', [1]),\n",
       " ('the', [1, 1, 1]),\n",
       " ('wood', [1, 1]),\n",
       " ('woodchuck', [1, 1]),\n",
       " ('would', [1])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2: some magic happens (ignore this line of code, as it's\n",
    "#         implicit in whatever map/reduce framework you use\n",
    "import itertools\n",
    "collated_word_instances = [\n",
    "    (k, [x[1] for x in v])\n",
    "    for k, v in itertools.groupby(sorted(word_instances), lambda x: x[0])\n",
    "]\n",
    "\n",
    "# now we have each word followed by a list of values to be summed\n",
    "collated_word_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('over', 1),\n",
       " ('chuck', 2),\n",
       " ('seashore', 1),\n",
       " ('sea', 1),\n",
       " ('sells', 1),\n",
       " ('if', 1),\n",
       " ('would', 1),\n",
       " ('shells', 1),\n",
       " ('fox', 1),\n",
       " ('how', 1),\n",
       " ('much', 1),\n",
       " ('woodchuck', 2),\n",
       " ('brown', 1),\n",
       " ('lazy', 1),\n",
       " ('jumps', 1),\n",
       " ('by', 1),\n",
       " ('a', 2),\n",
       " ('wood', 2),\n",
       " ('could', 1),\n",
       " ('dog', 1),\n",
       " ('she', 1),\n",
       " ('quick', 1),\n",
       " ('the', 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3: for each word, sum the counts\n",
    "word_counts = {}\n",
    "for word, counts in collated_word_instances:\n",
    "    word_counts[word] = sum(counts)\n",
    "\n",
    "word_counts.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this code is less efficient than the previous solution, it has the characteristic that phase 1 can be run on separate documents by different computers and the full output can be sent to a single computer for summing and reporting the final output in phase 3.\n",
    "\n",
    "Phase 2 can be quite complex: there's data collation and orchestrating the return of intermediate values to the reducer(s). Also, at scale there would be a lot of intermediate processing of data to greatly reduce the amount of data that actually has to be sent back to the server putting the final answer together, but that's beyond the scope of this lesson. Luckily, you can treat phase 2 as a \"magic step\" and still use the paradigm successfully.\n",
    "\n",
    "<img src=\"./step2.gif\" />\n",
    "\n",
    "Putting it all together into a map phase and a reduce phase (and trusting the framework handles the collation phase for us):\n",
    "\n",
    "```python\n",
    "def mapfn(key, value):\n",
    "    '''\n",
    "    receive documents and output individual word count tuples\n",
    "       \n",
    "    key: some kind of identifier, like a path to the source file\n",
    "    value: the body of text indicated by the key\n",
    "    '''\n",
    "    for word in value.split():\n",
    "        yield (word, 1)\n",
    "\n",
    "def reducefn(key, value):\n",
    "    '''\n",
    "    sum the counts for each word\n",
    "       \n",
    "    key: the word\n",
    "    value: the list of counts emitted by multiple mapfn calls\n",
    "    '''\n",
    "    yield key, sum(value)\n",
    "```\n",
    "\n",
    "Now our entire algorithm can be expressed in 2 functions for a total of five lines of code to get the same `word_counts` list generated previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Pi\n",
    "\n",
    "As has been aluded to earlier, pi can be computed by throwing darts at a dartboard. More specifically, if you randomly put points in a square of dimension `r` and count the number of points that are inside a radius of `r` from the origin vs the total number of points generated, you can compute pi as:\n",
    "\n",
    "```python\n",
    "pi_approximation = points_within_one_radius / total_points\n",
    "```\n",
    "\n",
    "For a comprehensive analysis of why this is true, see [Estimating the value of Pi using Monte Carlo](https://www.geeksforgeeks.org/estimating-value-pi-using-monte-carlo/) and also [Wikipedia's description of the Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive method\n",
    "\n",
    "The naive method for computing pi in this manner is to just generate a whole bunch of random points and sum up the totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started computing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.14190636"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "random.seed(uuid.uuid4())\n",
    "total = 100000000\n",
    "\n",
    "print('started computing...')\n",
    "inside = sum(math.sqrt(random.random() ** 2 + random.random() ** 2) <= 1. for i in range(total))\n",
    "\n",
    "# multiply by 4 because our \"square in a circle\" is only one quadrant\n",
    "4. * inside / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 30 seconds for my computer to compute this for 100,000,000 points, so it would take roughly an hour to compute ten billion points. We want to go faster!\n",
    "\n",
    "### Map\n",
    "\n",
    "A map function that generates `k` random points and counts how many are inside vs. outside one radius from the origin, returning a key of \"totals\" and a value of (num_inside, total_count):\n",
    "\n",
    "```python\n",
    "def mapfn(key, value):\n",
    "    '''compute `value` random points and return count of inside radius and total points'''\n",
    "    random.seed(uuid.uuid4())\n",
    "    inside = sum(\n",
    "        math.sqrt(random.random() ** 2 + random.random() ** 2) < 1.\n",
    "        for i in xrange(value)\n",
    "    )\n",
    "    yield 'totals', (inside, value)\n",
    "```\n",
    "\n",
    "Note that this function will be run many times on many computers, generating a very long list of counts (one for each map task).\n",
    "\n",
    "### Reduce\n",
    "\n",
    "A reduce function that sums up the totals computed by all the map functions:\n",
    "\n",
    "```python\n",
    "def reducefn(key, value):\n",
    "    inside = 0\n",
    "    total = 0\n",
    "    for v in value:\n",
    "        inside += v[0]\n",
    "        total += v[1]\n",
    "    return inside, total\n",
    "```\n",
    "\n",
    "The final totals that the reduce function emits are the sum of *all* points inside one radius and the sum of *all* darts thrown.\n",
    "\n",
    "Putting that into our earlier formula, we get:\n",
    "\n",
    "```python\n",
    "pi_approximation = 4.0 * inside / total\n",
    "```\n",
    "\n",
    "### One More Optimization\n",
    "\n",
    "It happens to be that we can take out one of the most expensive math functions we are computing and still get the same answer. Since `math.sqrt(1.0) == 1.0` and since any sum that is less than `1.0` has a square root of less than `1.0` and any number that is greater than `1.0` has a square root that is also greater than `1.0`, we do not need to take the square root:\n",
    "\n",
    "```python\n",
    "def mapfn(key, value):\n",
    "    ...\n",
    "    inside = sum(\n",
    "        (random.random() ** 2 + random.random() ** 2) < 1.\n",
    "        for i in xrange(value)\n",
    "    )\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "Let's do this! I'm going to ask all of you to help me compute pi. We are going to request 1000 map tasks, each throwing 10,000,000 darts. That's *ten billion* darts we are going to throw, which should give us a value for pi accurate to about six or seven digits.\n",
    "\n",
    "The code defines a map function and a reduce function, and then it starts up the server. It will simply sit and wait for workers to connect:\n",
    "\n",
    "```bash\n",
    "$ python -m pi \n",
    "data: {1: 10000000, 2: 10000000, 3: 10000000, ..., 998: 10000000, 999: 10000000, 1000: 10000000}\n",
    "waiting for workers...\n",
    "```\n",
    "\n",
    "When a worker connects, the server sends it the function to execute followed by tasks one at a time as the worker reports that it is ready for a task.\n",
    "\n",
    "\n",
    "```bash\n",
    "shell 1> python -m mincemeat -p pass localhost\n",
    "mapfn: 1, 10000000\n",
    "mapfn: 2, 10000000\n",
    "mapfn: 3, 10000000\n",
    "...\n",
    "```\n",
    "\n",
    "I can start a second worker, and it will pick up whatever task is next and then the two will basically split the work:\n",
    "\n",
    "```bash\n",
    "shell 2> python -m mincemeat -p pass localhost\n",
    "mapfn: 4, 10000000\n",
    "mapfn: 6, 10000000\n",
    "mapfn: 8, 10000000\n",
    "...\n",
    "```\n",
    "\n",
    "Once all the tasks have been completed (regardless of which worker completed each task), the server then sends out the single reduce job to one of the workers. The server gets back the result and prints a final answer to the screen.\n",
    "\n",
    "```bash\n",
    "results: {'totals': (78546246, 100000000)}\n",
    "totals: 78546246 inside, 100000000 total, pi ~= 3.14184984\n",
    "```\n",
    "\n",
    "See the [complete source code](./pi.py)\n",
    "\n",
    "When I run this on my 4-processor laptop with 4 task runners running in parallel, it takes 13.9 minutes to complete.\n",
    "\n",
    "For you to help me run it:\n",
    "\n",
    "1. install or download `mincemeat`\n",
    "\t * `pip install mincemeat` inside a virtual environment\n",
    "\t * ... or just download `mincemeat.py` from [https://bit.ly/2JVFIqR](https://bit.ly/2JVFIqR)\n",
    "1. once my server is started, run mincemeat: `python -m mincemeat -p pass <server address>`\n",
    "    * note that `mincemeat` only works with python 2... you might need to specify the python version when running the command: `python2 -m mincemeat -p pass <server address>`\n",
    "    * note that you can start multiple workers, but your computer will only be efficient running no more workers than you have CPU cores (this particular algorithm is CPU-bound, so context switching within the CPU far outweighs any benefit gained by trying to run additional workers)\n",
    "    * watch as your computer shows job numbers it is running\n",
    "\n",
    "Ready, go!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
